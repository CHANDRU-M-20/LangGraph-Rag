{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2727fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8697fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Why “Fast” Matters for Language Models\n",
      "\n",
      "| Category | Why speed is critical | Typical consequences of slow inference |\n",
      "|----------|-----------------------|----------------------------------------|\n",
      "| **User experience** | Real‑time or near‑real‑time responses are expected in chatbots, voice assistants, gaming, and AR/VR. | Delayed replies make interfaces feel clunky, increase abandonment, and hurt brand perception. |\n",
      "| **Cost & scalability** | Each token costs compute time and energy. Higher throughput means lower per‑token cost and the ability to serve more users on the same hardware. | Large‑scale deployments (search, recommendation, compliance) become prohibitively expensive. |\n",
      "| **Edge & embedded AI** | Devices such as smartphones, wearables, autonomous vehicles, and IoT sensors often lack GPUs or rely on low‑power CPUs. | Without fast, lightweight models, many applications cannot run locally, forcing latency‑intensive cloud calls. |\n",
      "| **Safety & control** | Prompt responses are needed for safety‑critical systems (e.g., medical triage, emergency dispatch). | Slow inference could delay crucial interventions and expose users to risk. |\n",
      "| **Iterative development** | Faster inference means more experiments, quicker fine‑tuning cycles, and faster A/B testing. | Longer turnaround hampers innovation and slows feature releases. |\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Core Motivations\n",
      "\n",
      "### 1.1 Latency vs. Throughput\n",
      "\n",
      "| Metric | What it tells you | Why it matters |\n",
      "|--------|-------------------|----------------|\n",
      "| **Latency** (ms per request) | The waiting time a user experiences. | Directly visible to the user. |\n",
      "| **Throughput** (tokens/s) | How many tokens you can process in a second. | Determines how many simultaneous users you can support. |\n",
      "\n",
      "Most commercial products aim for *sub‑500 ms* latency on a single user request while maintaining *tens of thousands of tokens per second* across all users. Achieving both requires careful engineering: model design, hardware optimization, and software pipeline tuning.\n",
      "\n",
      "### 1.2 Cost Per Token\n",
      "\n",
      "- **Cloud inference**: GPUs cost ~$3–$6 per hour. A 10‑ms inference on a large GPT‑4 sized model could cost ~\\$0.0001 per token. Multiply by millions of tokens per day → significant revenue impact.\n",
      "- **Edge deployment**: Battery life, heat, and device cost directly depend on power consumption, which is a function of inference speed.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Technical Pathways to Speed\n",
      "\n",
      "### 2.1 Model Design & Architecture\n",
      "\n",
      "| Approach | How it speeds things up | Trade‑offs |\n",
      "|----------|------------------------|------------|\n",
      "| **Efficient Transformer variants** (e.g., Longformer, Performer, Reformer) | Reduce self‑attention complexity from O(N²) to O(N) or O(N log N). | May introduce approximation errors; less expressive for long contexts. |\n",
      "| **Sparse attention** | Compute attention only for a subset of token pairs. | Can degrade quality if important interactions are pruned. |\n",
      "| **Layer‑wise pruning** | Remove entire transformer layers after fine‑tuning. | Risk of losing capacity; may need domain‑specific knowledge. |\n",
      "\n",
      "### 2.2 Quantization & Compression\n",
      "\n",
      "| Method | Speed gains | Impact |\n",
      "|--------|-------------|--------|\n",
      "| **Post‑training 8‑bit quantization** | 2–4× faster matrix ops, lower memory bandwidth. | Minimal loss in accuracy for many LLMs. |\n",
      "| **Dynamic quantization** | On‑the‑fly 8‑bit conversion; works on CPU. | Slight accuracy drop but no training cost. |\n",
      "| **Weight pruning** (structured or unstructured) | Reduces FLOPs; unstructured needs GPU support. | Requires careful fine‑tuning. |\n",
      "\n",
      "### 2.3 Knowledge Distillation\n",
      "\n",
      "- **Teacher‑student paradigm**: Train a smaller “student” to mimic a large “teacher.”  \n",
      "- **Benefits**: Up to 5–10× faster while retaining 90–95% of original performance.  \n",
      "- **Limitations**: Distillation requires an expensive teacher inference cost and a well‑aligned loss.\n",
      "\n",
      "### 2.4 Hardware & Software Stack\n",
      "\n",
      "| Layer | Optimization | Typical Speedups |\n",
      "|-------|--------------|------------------|\n",
      "| **GPU/TPU** | Mixed‑precision FP16/FP8, Tensor Cores | 2–8× over FP32 |\n",
      "| **CPU** | Vector extensions (AVX‑512), AVX‑Neon, QAT (quantized acceleration) | 1–4× over naive implementation |\n",
      "| **Inference libraries** | FlashAttention, TensorRT, ONNX Runtime, Triton | 2–10× over naive attention |\n",
      "| **Edge accelerators** | ASICs (e.g., Google Edge TPU, Apple Neural Engine) | 10–100× over CPU, with low power draw |\n",
      "\n",
      "### 2.5 Dynamic & Adaptive Computation\n",
      "\n",
      "- **Early exiting**: Stop inference once the model is confident.  \n",
      "- **Adaptive token length**: Process fewer tokens for short responses.  \n",
      "- **Benefits**: Significant latency reductions on average use‑cases while keeping worst‑case bounds acceptable.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Real‑World Use Cases\n",
      "\n",
      "| Domain | Speed requirement | Example fast‑LLM in production |\n",
      "|--------|-------------------|--------------------------------|\n",
      "| **Chatbots & Virtual Assistants** | < 300 ms per turn | Mistral 7B + FastChat, Llama-2-7B + ONNX |\n",
      "| **Search & Retrieval Augmentation** | < 200 ms per query | Mixtral 8×7B on Triton, DistilBERT for ranking |\n",
      "| **Gaming & Real‑time NPC dialogue** | < 100 ms | FastGPT‑3.5 (quantized), GPT‑NeoX |\n",
      "| **Healthcare triage** | < 500 ms | BioGPT‑2B quantized on CPU |\n",
      "| **Autonomous Vehicles** | < 50 ms | TinyCLIP + LLM for sensor fusion, edge TPU |\n",
      "| **IoT & Smart Home** | < 200 ms | Edge‑optimized Llama-2-3B on Raspberry Pi 4 |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Balancing Speed and Accuracy\n",
      "\n",
      "| Strategy | When to use it? | Caveats |\n",
      "|----------|----------------|---------|\n",
      "| **Model pruning** | Need to keep 95–99% of accuracy, but reduce size. | Requires careful validation to avoid catastrophic forgetting. |\n",
      "| **Quantization** | Deploy on CPUs, edge, or when cost is a constraint. | Some models (e.g., very large transformers) exhibit larger accuracy drops at 8‑bit. |\n",
      "| **Distillation** | Want a new model trained offline that runs on low‑power hardware. | Distillation pipeline adds engineering overhead. |\n",
      "| **Dynamic inference** | Applications with variable length inputs and tolerant latency. | Implementation complexity; need a robust confidence metric. |\n",
      "\n",
      "Often the best solution is a *hybrid*: a pruned, quantized transformer that also uses dynamic early‑exiting for short responses.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Practical Guidelines for Building a Fast LLM Service\n",
      "\n",
      "1. **Profile Early**  \n",
      "   * Measure latency per token on the target hardware.  \n",
      "   * Identify bottlenecks: self‑attention, feed‑forward, IO.\n",
      "\n",
      "2. **Choose the Right Architecture**  \n",
      "   * If you need long‑context (10k+ tokens), consider Performer/Longformer.  \n",
      "   * For short‑context chat, a dense transformer + FlashAttention can suffice.\n",
      "\n",
      "3. **Apply Quantization**  \n",
      "   * Start with post‑training 8‑bit; test 4‑bit only if accuracy remains acceptable.  \n",
      "   * Use quantization‑aware training for critical applications.\n",
      "\n",
      "4. **Distill if Needed**  \n",
      "   * Distill a 30B teacher to a 7B or 3B student.  \n",
      "   * Use knowledge‑distillation loss + confidence‑weighted cross‑entropy.\n",
      "\n",
      "5. **Optimize the Runtime**  \n",
      "   * Deploy with TensorRT or Triton for GPU/TPU acceleration.  \n",
      "   * Use ONNX Runtime with QLinear ops for CPU.  \n",
      "\n",
      "6. **Use Dynamic Techniques**  \n",
      "   * Early exit after layer 6 for confident responses.  \n",
      "   * Reduce beam width for decoding when latency is critical.\n",
      "\n",
      "7. **Monitor & Retrain**  \n",
      "   * Continuously track latency, throughput, and accuracy.  \n",
      "   * Retrain or fine‑tune if drift occurs due to new data or usage patterns.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Future Directions\n",
      "\n",
      "- **Neural Architecture Search (NAS) for speed**: Auto‑design of transformer variants tuned for a target device.  \n",
      "- **Neuro‑Sparsity**: Learn sparsity patterns end‑to‑end for maximal speed.  \n",
      "- **Hardware‑Software Co‑design**: New accelerators (e.g., GPT‑specific ASICs) that support fused attention, quantized ops, and early‑exit logic.  \n",
      "- **Model‑agnostic Inference APIs**: Unified runtime that automatically selects the best execution plan based on current load.  \n",
      "- **Explainable Speed‑up**: Tools that explain which tokens or layers are most costly in a given inference.\n",
      "\n",
      "---\n",
      "\n",
      "## Bottom Line\n",
      "\n",
      "Fast language models are no longer a nicety—they’re a business imperative. They unlock:\n",
      "\n",
      "- **Seamless user experience** (sub‑second responses).  \n",
      "- **Competitive advantage** (lower cost per token, higher throughput).  \n",
      "- **Broader accessibility** (edge deployment, low‑budget devices).  \n",
      "- **Safety & reliability** (quick decisions in time‑critical scenarios).\n",
      "\n",
      "By combining efficient transformer designs, compression techniques, smart software pipelines, and hardware acceleration, you can build systems that deliver state‑of‑the‑art NLP capabilities **without compromising on speed**.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    ")\n",
    "\n",
    "response = client.responses.create(\n",
    "    input=\"Explain the importance of fast language models\",\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    ")\n",
    "print(response.output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5827fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0be5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    graph_info:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c9d5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_play(state:State):\n",
    "    print(\"Start - play function calling\")\n",
    "    return {\n",
    "        \"graph_info\":state['graph_info'] + \"I am planning to start play\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "def cricket(state:State):\n",
    "    print(\"my cricket node has been called\")\n",
    "    return {\n",
    "        \"graph_info\":state['graph_info'] + \" Cricket\"\n",
    "    }\n",
    "    \n",
    "\n",
    "def Badmiton(state:State):\n",
    "    print(\"my cricket node has been called\")\n",
    "    return {\n",
    "        \"graph_info\":state['graph_info'] + \" Badmiton\"}\n",
    "\n",
    "\n",
    "import random \n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def randplay(state:State)->Literal['cricket','badmiton']:\n",
    "    graph_info = state['graph_info']\n",
    "    if random.random()>0.5:\n",
    "        return \"cricket\"\n",
    "    else:\n",
    "        return \"badmiton\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88913960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image,display\n",
    "from langgraph.graph import StateGraph,END,START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40adf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cf9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
